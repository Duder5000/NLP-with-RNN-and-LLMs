{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duder5000/NLP-with-RNN-and-LLMs/blob/main/Text_generation_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware accelerator > GPU*.\n",
        "\n",
        "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/guide/keras/sequential_model) and [eager execution](https://www.tensorflow.org/guide/eager). The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "outputId": "d55025ad-92f7-4b29-a3fe-5c38272e1856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aavnuByVymwK",
        "outputId": "ace0c0b6-b0ab-42ff-f0e4-c26be7f77713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Duhg9NrUymwO"
      },
      "outputs": [],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "#print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "outputId": "fbc65664-bf6b-425d-d3f7-81e22da80e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a86OoYtO01go",
        "outputId": "6e65a783-e906-4f2c-9020-2c75f637d254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "outputId": "5c050602-1230-43a0-e67a-71780781a017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "outputId": "0f4a32d8-3eb6-4703-e2e1-b515f0bfac92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "outputId": "258db070-22fe-41d6-acac-d7fe1f7f61ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UopbsKi88tm5",
        "outputId": "0a12a6a3-6b71-4fd3-8bc2-53734029d837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "outputId": "e63259d7-8951-4030-a33e-e2b5150eb096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "outputId": "32dff721-1252-4f3a-a8a5-bd76c6469e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QO32cMWu4a06",
        "outputId": "cf5fbd44-53a6-4394-a889-1b69fba5e75d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and\n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "outputId": "3a6874c5-0696-4bd2-d1e4-376f2d3ddb72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "outputId": "6bc44bf0-21f0-4b5c-9a0d-011fc835e45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "outputId": "2faaef42-3240-4d4b-c85c-a4f560349bed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "outputId": "1b2953bb-d186-4394-f4a9-a8df2a7a4dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "outputId": "99d2801f-2958-49d8-defb-3d10cb62a5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices.shape"
      ],
      "metadata": {
        "id": "Vu83Q4vuUm19",
        "outputId": "3c5d62d9-d4c1-4ee6-afe6-669c8c8a3a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "outputId": "f29d5c0a-fe60-452f-e9f7-e0ecdef677de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([47, 36, 33, 24, 56, 50, 18,  1, 64, 34, 51, 39, 50, 58,  2, 14, 32,\n",
              "        5, 48, 17,  6, 10, 31, 19, 54, 48, 15,  9, 42, 52, 59, 35, 54, 37,\n",
              "       10, 38, 64,  3, 42, 25, 28, 41, 32, 42, 40, 51,  3, 53, 56,  6, 52,\n",
              "       21, 52, 65, 16, 43, 23, 28, 35, 45, 34, 44, 64,  0, 45, 63,  6, 65,\n",
              "       39, 21, 17,  3, 38, 60, 34, 49, 53, 36,  4, 35, 42, 49,  6, 55, 44,\n",
              "       64, 32, 12, 59, 46, 21, 23, 15,  6, 39, 48, 36, 62, 26, 59])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "outputId": "f271379a-1626-4330-966b-fe72a31f208e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'with him, the portion and sinew of\\nher fortune, her marriage-dowry; with both, her\\ncombinate husband'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"hWTKqkE\\nyUlZks AS&iD'3RFoiB.cmtVoX3Yy!cLObScal!nq'mHmzCdJOVfUey[UNK]fx'zZHD!YuUjnW$Vcj'peyS;tgHJB'ZiWwMt\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "outputId": "ff37b2d6-ceae-4097-8522-647879f88134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1889195, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output\n",
        "logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "outputId": "ec1c0cba-8766-4494-be04-15eb42037183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.95149"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "outputId": "91c73300-51ca-479f-c171-ea50ba7d344a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 2.7231\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.9899\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.7104\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.5484\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.4487\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3814\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.3281\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2840\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2428\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2035\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1629\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1220\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0796\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0330\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9848\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9338\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8810\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8301\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7783\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7298\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "outputId": "a613042a-922b-40d1-85ff-5ad5619e0229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "This grieves, my lord, as your good worship back\n",
            "With all the swift past trongs upon thine.\n",
            "\n",
            "ROMEO:\n",
            "Is the great damning thing! most dimbrime me\n",
            "Worst of tricu? it is stark on her frank?\n",
            "\n",
            "BIANCA:\n",
            "That made these bartal-steels here, then must I called\n",
            "by\n",
            "Warwick's steed lend face and frown upon mine own\n",
            "suspicion.\n",
            "\n",
            "BRUTUS:\n",
            "One garments, boy'd brother, prince,\n",
            "As little feast, sir, nobly father, I\n",
            "what nothing can no more of ignifally makes thee!\n",
            "\n",
            "Lord Mayor:\n",
            "Go blussed with him, the first care ranged:\n",
            "and what he which thou sweet'st a spirit too?\n",
            "\n",
            "KING RICHARD III:\n",
            "Which once arrivakes; and it conceal'd\n",
            "English wrings from banishment; for this unsoluments\n",
            "Come from your kindness should entreaty.\n",
            "I have found so many midels in the people,\n",
            "As I guilty deeds. Is who is arms\n",
            "I'll practil gentlemen to our provoke.\n",
            "\n",
            "KING EDWARD IV:\n",
            "This peremptory Kate the clouds of conclusing peace.\n",
            "Now so whose fives, sir, to whose fiver counsels\n",
            "And sendsh concords against the Tower.\n",
            "\n",
            "MOMPAAE:\n",
            "Sir, I than \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.129117965698242\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "outputId": "1e39ebff-70ab-4ab9-a55a-fcc2468a39ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nO my double wit! what hap denich then weep I take you,\\nLeave and one tap to do itself: he is a father's husband\\nBy your love's dead, and yours, and at fight,\\nWhere my patried enditany and right;\\nThey call be leave it but a phison,\\nThat swear it from ensirving whose inward as\\nany thing; we should break in approbation\\nWith what repair doth charges him to the Volsces,\\nNor that will be a paised of concerns with greatness:\\nLion and again why then, Warwick was the wind;\\nNow you can be come done, still terror it.\\n\\nRICHARD:\\nI'll jain behold this cur drefs.\\n\\nSecond Senator:\\nFaith, let her and never write. For in abred what\\nwith that woes with war with me who to, good sweet Iur.\\n\\nSecond Senator:\\nNow I fear thee, save you by that woddy\\nAs 'twere a broken depation, to your father's land\\nSuch in my piteous and another froward\\nOf backs both particularry conform always\\ngrows no great ear lord. I have yet accuse him, when the king\\nThat I will call him hence, and all thrice with\\ngrateful instruments, \"\n",
            " b\"ROMEO:\\nI long to with my fair entir; and then the great\\nAs all the light.\\n\\nSICINIUS:\\nWhat's the matter?\\n\\nMENENIUS:\\nYour worth are going:\\nTo pray for, since the king hath broken filly.\\nIn brother, for tith once of benefit\\nWe can: both from my Hewrust saw yon\\nThe latest defend him that did spain itself\\nAs I could find misfortacting creent.\\nThe dark not title and the thing I believe,\\nI have, my lord.\\n\\nDUKE VINCENTIO:\\n'Twixt not that I must way so unlawfully.\\n\\nYORK:\\nWhy do apparent learning and destroy'd\\nAnd craves nothing unso wife, you need of him\\nA mile-known supperate men to you.\\n\\nBISHOP OF CARLISLE:\\nMy gracious sovereign, and in hoon?\\n\\nKING RICHARD III:\\nRepure, I know, my lord.\\n\\nDUKE VINCENTIO:\\nThat will I watch the county slain.\\nAnd yet the banish reins and singled virtue.\\n\\nLADY GREY:\\nThen, mightst thou be talked on for and do it please.\\nIn the love men, 'tis desired or fruit.\\nNay, keep your lord of Brogged: be best.\\n\\nMOPSA:\\nI was, by him in justice, which thou art mall\\nWith thy abundant c\"\n",
            " b\"ROMEO:\\nNothing but business, do, and fetch so fair,\\nSo every shall be older, by their grave,\\nLover freely gave thee granted: sever, and speaks\\nIf to be able to men in steel.\\n\\nGLOUCESTER:\\nYet love the moveless wife, shall I go with you.\\nCome, ladies fore! slandering arguments;\\nO'ercomps my husband-manifes and the senated\\nThat are but told his heir more hat for a doin.\\nWhat cheer is this, that with his grace objuduct\\nWhich is't watch with him?\\n\\nPOMPELEY:\\nMenry, desperate knife, that wend on name I speak.\\nYour praises, sir, she bears me. Yet, as\\nthey have, my lord; and am so deep an instrument,\\nAnd hither 'twixt thy falsehabilate, and, speaking,\\nThough he begett a thousand lords to see,\\nWhen I said, thus to sep me in my tree,\\nAnd over the lustful uncle dukedom I omith\\nThee for a balm could new brothers ne'er come of\\nThinking--thus have accused from my guiltless\\nare our general as the deputy ingegion,\\nOr with the banish'd decence and an ene:\\nThe peace you say ye, then, I am sent for thee.\\n\\nPost:\"\n",
            " b\"ROMEO:\\nThou detestab's me! I, that thy most kind cobles me\\nTo learn the grace of God and think\\nHow God forbid the lists of tick-tackle-forth,\\nFor joyful bower grod in marriage,\\nOr equal that with our woes with Lady Lay\\nAs I do put the word: then.\\nHere's our dearers than dark clofations: thou art\\nset to-night; to wage, with lions comp it:\\nTo-day should see all purses vain floted,\\nand an engranchforgonest freely stands 'Twixt myself,\\nIf not, revenged that he should be,\\nSent it's return to thy ancient disposition,\\nAnd then expect refeen to Richard spake.\\n\\nGLOUCESTER:\\nWell thou art much better gentle word before a nearing.\\n\\nDUKE:\\nMenderery, arry with me a biar.\\n\\nSecond Murderer:\\nNone, but only!\\n\\nISABELLA:\\nAnd is thus too?\\n\\nFLORIZEL:\\nI mind to you.\\n\\nDUCHESS OF YORK:\\nSo shall your letter presugges thy foot.\\n\\nMessenger:\\nWhat cannot we, to arm and sleep or cepturing he gives my need hearing--\\n\\nADABOLLA:\\nThe other sister Katharina,\\nAnd in the great sovereignty of healths\\nTo rechin her ears and marine\"\n",
            " b\"ROMEO:\\n\\nGREMIO:\\nTo hear, sir, my turn your gage: but I be not,\\nWhen it was made given to be but sworn in a\\nday both, and be preving now?\\nShamed your father sidng, and am not\\nMany another man from all the spirits of thy fame;\\nWas not full of his free heart\\nMake methour kinsmen gapposed with his ears:\\nit is well inclined to me for his life,\\nWho would your sentence of his shoulders\\nThan place the senselers to us all, your mother lives\\nSo many fills that will stin he call me hence;\\nAnd, for, with nimble man; I kneel\\nAnd graftil grief in grief and such as breast,\\nBut first being barren a misserina turn to you,\\nWhere ne'er she is against their glories. Then, canon\\nDid what he reign beloved in blood?\\n\\nNurse:\\nYour ship or never match, methinks.\\n\\nSICINIUS:\\nNote is already displess burn and to grant\\nTo happy things as yours,\\nSo many shoulder seas on me; besides,\\nSweet father, I did know.\\n\\nKING LEWIS XI:\\nAnd let him speak, like heavy wit, by blistery.\\n\\nGLOUCESTER:\\nIt is for you to do he, for never give\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1019928455352783\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "#print(result.numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(result)):\n",
        "  print(result[i].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n"
      ],
      "metadata": {
        "id": "IHNAtoGXfylV",
        "outputId": "eee34ffc-2422-4d19-e673-a8321143a381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "O my double wit! what hap denich then weep I take you,\n",
            "Leave and one tap to do itself: he is a father's husband\n",
            "By your love's dead, and yours, and at fight,\n",
            "Where my patried enditany and right;\n",
            "They call be leave it but a phison,\n",
            "That swear it from ensirving whose inward as\n",
            "any thing; we should break in approbation\n",
            "With what repair doth charges him to the Volsces,\n",
            "Nor that will be a paised of concerns with greatness:\n",
            "Lion and again why then, Warwick was the wind;\n",
            "Now you can be come done, still terror it.\n",
            "\n",
            "RICHARD:\n",
            "I'll jain behold this cur drefs.\n",
            "\n",
            "Second Senator:\n",
            "Faith, let her and never write. For in abred what\n",
            "with that woes with war with me who to, good sweet Iur.\n",
            "\n",
            "Second Senator:\n",
            "Now I fear thee, save you by that woddy\n",
            "As 'twere a broken depation, to your father's land\n",
            "Such in my piteous and another froward\n",
            "Of backs both particularry conform always\n",
            "grows no great ear lord. I have yet accuse him, when the king\n",
            "That I will call him hence, and all thrice with\n",
            "grateful instruments,  \n",
            "\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "I long to with my fair entir; and then the great\n",
            "As all the light.\n",
            "\n",
            "SICINIUS:\n",
            "What's the matter?\n",
            "\n",
            "MENENIUS:\n",
            "Your worth are going:\n",
            "To pray for, since the king hath broken filly.\n",
            "In brother, for tith once of benefit\n",
            "We can: both from my Hewrust saw yon\n",
            "The latest defend him that did spain itself\n",
            "As I could find misfortacting creent.\n",
            "The dark not title and the thing I believe,\n",
            "I have, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "'Twixt not that I must way so unlawfully.\n",
            "\n",
            "YORK:\n",
            "Why do apparent learning and destroy'd\n",
            "And craves nothing unso wife, you need of him\n",
            "A mile-known supperate men to you.\n",
            "\n",
            "BISHOP OF CARLISLE:\n",
            "My gracious sovereign, and in hoon?\n",
            "\n",
            "KING RICHARD III:\n",
            "Repure, I know, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "That will I watch the county slain.\n",
            "And yet the banish reins and singled virtue.\n",
            "\n",
            "LADY GREY:\n",
            "Then, mightst thou be talked on for and do it please.\n",
            "In the love men, 'tis desired or fruit.\n",
            "Nay, keep your lord of Brogged: be best.\n",
            "\n",
            "MOPSA:\n",
            "I was, by him in justice, which thou art mall\n",
            "With thy abundant c \n",
            "\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "Nothing but business, do, and fetch so fair,\n",
            "So every shall be older, by their grave,\n",
            "Lover freely gave thee granted: sever, and speaks\n",
            "If to be able to men in steel.\n",
            "\n",
            "GLOUCESTER:\n",
            "Yet love the moveless wife, shall I go with you.\n",
            "Come, ladies fore! slandering arguments;\n",
            "O'ercomps my husband-manifes and the senated\n",
            "That are but told his heir more hat for a doin.\n",
            "What cheer is this, that with his grace objuduct\n",
            "Which is't watch with him?\n",
            "\n",
            "POMPELEY:\n",
            "Menry, desperate knife, that wend on name I speak.\n",
            "Your praises, sir, she bears me. Yet, as\n",
            "they have, my lord; and am so deep an instrument,\n",
            "And hither 'twixt thy falsehabilate, and, speaking,\n",
            "Though he begett a thousand lords to see,\n",
            "When I said, thus to sep me in my tree,\n",
            "And over the lustful uncle dukedom I omith\n",
            "Thee for a balm could new brothers ne'er come of\n",
            "Thinking--thus have accused from my guiltless\n",
            "are our general as the deputy ingegion,\n",
            "Or with the banish'd decence and an ene:\n",
            "The peace you say ye, then, I am sent for thee.\n",
            "\n",
            "Post: \n",
            "\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "Thou detestab's me! I, that thy most kind cobles me\n",
            "To learn the grace of God and think\n",
            "How God forbid the lists of tick-tackle-forth,\n",
            "For joyful bower grod in marriage,\n",
            "Or equal that with our woes with Lady Lay\n",
            "As I do put the word: then.\n",
            "Here's our dearers than dark clofations: thou art\n",
            "set to-night; to wage, with lions comp it:\n",
            "To-day should see all purses vain floted,\n",
            "and an engranchforgonest freely stands 'Twixt myself,\n",
            "If not, revenged that he should be,\n",
            "Sent it's return to thy ancient disposition,\n",
            "And then expect refeen to Richard spake.\n",
            "\n",
            "GLOUCESTER:\n",
            "Well thou art much better gentle word before a nearing.\n",
            "\n",
            "DUKE:\n",
            "Menderery, arry with me a biar.\n",
            "\n",
            "Second Murderer:\n",
            "None, but only!\n",
            "\n",
            "ISABELLA:\n",
            "And is thus too?\n",
            "\n",
            "FLORIZEL:\n",
            "I mind to you.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "So shall your letter presugges thy foot.\n",
            "\n",
            "Messenger:\n",
            "What cannot we, to arm and sleep or cepturing he gives my need hearing--\n",
            "\n",
            "ADABOLLA:\n",
            "The other sister Katharina,\n",
            "And in the great sovereignty of healths\n",
            "To rechin her ears and marine \n",
            "\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "\n",
            "GREMIO:\n",
            "To hear, sir, my turn your gage: but I be not,\n",
            "When it was made given to be but sworn in a\n",
            "day both, and be preving now?\n",
            "Shamed your father sidng, and am not\n",
            "Many another man from all the spirits of thy fame;\n",
            "Was not full of his free heart\n",
            "Make methour kinsmen gapposed with his ears:\n",
            "it is well inclined to me for his life,\n",
            "Who would your sentence of his shoulders\n",
            "Than place the senselers to us all, your mother lives\n",
            "So many fills that will stin he call me hence;\n",
            "And, for, with nimble man; I kneel\n",
            "And graftil grief in grief and such as breast,\n",
            "But first being barren a misserina turn to you,\n",
            "Where ne'er she is against their glories. Then, canon\n",
            "Did what he reign beloved in blood?\n",
            "\n",
            "Nurse:\n",
            "Your ship or never match, methinks.\n",
            "\n",
            "SICINIUS:\n",
            "Note is already displess burn and to grant\n",
            "To happy things as yours,\n",
            "So many shoulder seas on me; besides,\n",
            "Sweet father, I did know.\n",
            "\n",
            "KING LEWIS XI:\n",
            "And let him speak, like heavy wit, by blistery.\n",
            "\n",
            "GLOUCESTER:\n",
            "It is for you to do he, for never give \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "outputId": "43b9a8ac-33ce-491a-bc16-a209b21a0331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f26201df250>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "outputId": "fa76b2a4-8397-4d00-c438-3b217576d429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "You love this men, I have seen ' heard,\n",
            "Which often still like entertain; now art thou deceived;\n",
            "Sh\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "o694aoBPnEi9",
        "outputId": "855a4b4f-26b4-4a8f-a561-3aecc992d64a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 59ms/step - loss: 2.7162\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f25b6f779d0>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "outputId": "6a724a87-703f-4044-928a-c9152da20a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1872\n",
            "Epoch 1 Batch 50 Loss 2.0488\n",
            "Epoch 1 Batch 100 Loss 1.9642\n",
            "Epoch 1 Batch 150 Loss 1.8036\n",
            "\n",
            "Epoch 1 Loss: 1.9836\n",
            "Time taken for 1 epoch 13.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8321\n",
            "Epoch 2 Batch 50 Loss 1.7567\n",
            "Epoch 2 Batch 100 Loss 1.7052\n",
            "Epoch 2 Batch 150 Loss 1.5961\n",
            "\n",
            "Epoch 2 Loss: 1.7081\n",
            "Time taken for 1 epoch 11.55 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6151\n",
            "Epoch 3 Batch 50 Loss 1.5734\n",
            "Epoch 3 Batch 100 Loss 1.5326\n",
            "Epoch 3 Batch 150 Loss 1.4665\n",
            "\n",
            "Epoch 3 Loss: 1.5488\n",
            "Time taken for 1 epoch 11.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5314\n",
            "Epoch 4 Batch 50 Loss 1.4431\n",
            "Epoch 4 Batch 100 Loss 1.4505\n",
            "Epoch 4 Batch 150 Loss 1.4792\n",
            "\n",
            "Epoch 4 Loss: 1.4508\n",
            "Time taken for 1 epoch 11.65 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4257\n",
            "Epoch 5 Batch 50 Loss 1.4830\n",
            "Epoch 5 Batch 100 Loss 1.4036\n",
            "Epoch 5 Batch 150 Loss 1.3650\n",
            "\n",
            "Epoch 5 Loss: 1.3829\n",
            "Time taken for 1 epoch 11.44 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3312\n",
            "Epoch 6 Batch 50 Loss 1.3591\n",
            "Epoch 6 Batch 100 Loss 1.3311\n",
            "Epoch 6 Batch 150 Loss 1.3008\n",
            "\n",
            "Epoch 6 Loss: 1.3316\n",
            "Time taken for 1 epoch 11.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2814\n",
            "Epoch 7 Batch 50 Loss 1.3111\n",
            "Epoch 7 Batch 100 Loss 1.2919\n",
            "Epoch 7 Batch 150 Loss 1.2922\n",
            "\n",
            "Epoch 7 Loss: 1.2860\n",
            "Time taken for 1 epoch 11.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2367\n",
            "Epoch 8 Batch 50 Loss 1.2235\n",
            "Epoch 8 Batch 100 Loss 1.2513\n",
            "Epoch 8 Batch 150 Loss 1.2681\n",
            "\n",
            "Epoch 8 Loss: 1.2449\n",
            "Time taken for 1 epoch 11.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1926\n",
            "Epoch 9 Batch 50 Loss 1.1844\n",
            "Epoch 9 Batch 100 Loss 1.2251\n",
            "Epoch 9 Batch 150 Loss 1.1775\n",
            "\n",
            "Epoch 9 Loss: 1.2059\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1714\n",
            "Epoch 10 Batch 50 Loss 1.1626\n",
            "Epoch 10 Batch 100 Loss 1.1823\n",
            "Epoch 10 Batch 150 Loss 1.1769\n",
            "\n",
            "Epoch 10 Loss: 1.1658\n",
            "Time taken for 1 epoch 11.47 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}